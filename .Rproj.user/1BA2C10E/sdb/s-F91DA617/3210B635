{
    "contents" : "################################################################################*\n#  DATA CLEANING TEMPLATE\n################################################################################*\n\n#-------------------------------------------------------------------------------*\n# ---- SET-UP ----\n#===============================================================================*\n\n# Load libraries:\n\nlibrary(stringr)\nlibrary(plyr)\n\n# Source the functions file:\n\nsource('scripts/R-scripts/core-transient_functions.R')\n\n# Get data:\n\ngetwd()\n\nlist.files('data/raw_datasets')\n\ndataset = read.csv('data/raw_datasets/dataset_223.csv')\n\n#===============================================================================*\n# MAKE FORMATTED DATASET\n#===============================================================================*\n# The goal is: \n# 1) To create a dataset of the columns of interest at the smallest spatial\n#  and temporal sampling grain available.\n# 2) To eliminate \"bad\" species data.\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE THE DATASET ----\n#===============================================================================*\n# Here, you are predominantly interested in getting to know the dataset,\n# what the fields represent and determining which fields are relavent. Do \n# this carefully, making notes on a piece of scratch paper.\n\n# View field names:\n\nnames(dataset)\n\n# View how many records and fields:\n\ndim(dataset)\n\n# View the structure of the dataset:\n\nstr(dataset)\n\n# View first 6 rows of the dataset:\n\nhead(dataset)\n\n# Here, we can see that there are some fields that we won't use. Let's remove\n# them, note that I've given a new name here \"d1\", this is to ensure that\n# we don't have to go back to square 1 if we've miscoded anything.\n\nnames(dataset)\n\ndataset1 = dataset[,-c(5,6,8,9)]\n\nhead(dataset1)\n\n# Because all (and only) the fields we want are present, we can re-assign d1:\n\ndataset = dataset1\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE AND FORMAT SITE DATA ----\n#===============================================================================*\n\n# View summary of fields in the dataset:\n\nsummary(dataset)\n\n# Reminder of the dataset:\n\nhead(dataset)\n\n# We can see that sites are broken up into (potentially) 5 fields. Find the \n# metadata link in the data source table use that link to determine how\n# sites are characterized.\n#  -- If sampling is nested (e.g., site, block, treatment, plot, quad as in \n# this study), use each of the identifying fields and separate each field with\n# an underscore.\n# -- If sites are listed as lats and longs, use the finest available grain \n# and separate lat and long fields with an underscore.\n# -- If the site definition is clear, make a new site column as necessary.\n\n# Here, we will concatenate all of the potential fields that describe the \n# site:\n\nhead(dataset)\n\nsite = paste(dataset$site, dataset$block, dataset$treatment, \n             dataset$plot, dataset$quad, sep = '_')\n\n# Do some quality control by comparing the site fields in the dataset with the \n# new vector of sites:\n\nhead(site)\n\n# All looks correct, so replace the site column in the dataset (as a factor) \n# and remove the unnecessary fields, start by renaming the dataset in case \n# you make a mistake:\n\ndataset1 = dataset\n\ndataset1$site = factor(site)\n\ndataset1 = dataset1[,-c(2:5)]\n\n# Check the new dataset (are the columns as they should be?):\n\nhead(dataset1)\n\n# All looks good, so overwrite the dataset file:\n\ndataset = dataset1\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE SITE DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE AND FORMAT SPECIES DATA ----\n#===============================================================================*\n# Here, your primary goal is to ensure that all of your species are valid. To do\n# so, you need to look at the list of unique species very carefully. Avoid being\n# too liberal in interpretation, if you notice an entry that MIGHT be a problem, \n# but you can't say with certainty, create an issue on GitHub.\n\n# Look at the individual species present:\n\nsp = dataset$species\n\nlevels(sp) # Note: You can also use unique(sp) here.\n\n# The first thing that I notice is that there are lower and upper case\n# entries. Because R is case-sensitive, this will be coded as separate species.\n# Modify this prior to continuing:\n\ndataset$species = toupper(dataset$species)\n\n# Let's explore whether there was a difference:\n\nlength(unique(dataset$species))\n\nlength(unique(sp))\n\n# We see that almost 70 species were the result of upper and lower case!\n# Make a new species vector (factor ensures that it is coded as a factor\n# rather than character and removes any unused levels) \n# and continue exploring:\n\nsp = factor(dataset$species)\n\nlevels(sp)\n\n# Now explore the listed species themselves. To do so, you should go back to study's \n# metadata. A quick look at the metadata is not informative, unfortunately. Because of\n# this, you should really stop here and post an issue on GitHub. With some more thorough\n# digging, however, I've found the names represent \"Kartez codes\". Several species can\n# be removed (double-checked with USDA plant codes at plants.usda.gov and another Sevilleta\n# study (dataset 254) that provides species names for some codes). Some codes were identified\n# with this pdf from White Sands: \n# https://nhnm.unm.edu/sites/default/files/nonsensitive/publications/nhnm/U00MUL02NMUS.pdf\n\nbad_sp = c('', 'NONE','UK1','UKFO1','UNK1','UNK2','UNK3','LAMIA', 'UNGR1','CACT1','UNK','NONE',\n  'UNK2','UNK3', 'UNK1','FORB7', 'MISSING', '-888', 'DEAD','ERRO2', 'FORB1','FSEED', 'GSEED',\n  'MOSQ', 'SEED','SEEDS1','SEEDS2', 'SEFLF','SESPM','SPOR1')\n\ndataset1 = dataset[!dataset$species %in% bad_sp,]\n\ndataset1$species = factor(dataset1$species)\n\n# Let's look at how the removal of bad species altered the length of the dataset:\n\nnrow(dataset)\n\nnrow(dataset1)\n\n# Look at the head of the dataset to ensure everything is correct:\n\nhead(dataset1)\n\n# Having checked through the results, we can now reassign the dataset:\n\ndataset = dataset1\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE SPECIES DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE AND FORMAT TIME DATA ----\n#===============================================================================*\n# Here, we need to extract the sampling dates. \n\n# For starters, let's change the date column to a true date (and give the darned\n# column a better name:\n\nhead(dataset)\n\ndate = strptime(dataset$record_record_date, '%m/%d/%Y')\n\n# A check on the structure lets you know that date field is now a date object:\n\nclass(dataset$record_record_date)\n\nclass(date)\n\n# Give a double-check, if everything looks okay, then replace the column:\n\nhead(dataset$record_record_date)\n\nhead(date)\n\ndataset1 = dataset\n\ndataset1$record_record_date = date\n\nnames(dataset1)[5] = 'date'\n\n# Let's remove the season field (for now):\n\ndataset1 = dataset1[,-2]\n  \n# After a check of dataset1, you can rename it dataset:\n  \nhead(dataset)\n\nhead(dataset1)\n\ndataset = dataset1\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE DATE DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE AND FORMAT COUNT DATA ----\n#===============================================================================*\n# Next, we need to explore the count records. A good first pass is to remove \n# zero counts and NA's:\n\nsummary(dataset)\n\n# Subset to records > 0 (if applicable):\n\ndataset1 = subset(dataset, cover > 0) \n\nsummary(dataset1)\n\n# Remove NA's:\n\ndataset1 = na.omit(dataset1)\n\n# Make sure to write in the data summary table the type of observed count (here,\n# it represents % cover)\n\n# How does it look? If you approve,  assign changes to dataset:\n\nsummary(dataset)\nsummary(dataset1)\n\ndataset = dataset1\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE COUNT DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- MAKE DATA FRAME OF COUNT BY SITES, SPECIES, AND YEAR ----\n#===============================================================================*\n# Now we will make the final formatted dataset, add a datasetID field, check for\n# errors, and remove records that can't be used for our purposes.\n\n# First, lets add the datasetID:\n\ndataset1 = dataset\n\ndataset1$datasetID = rep(223,nrow(dataset1))\n\n# Change date to a factor:\n\ndataset1$date = factor(as.character(dataset1$date))\n\n# Now make the compiled dataframe:\n\ndataset2 = ddply(dataset1,.(datasetID, site, date, species),\n                 summarize, count = max(cover))\n\n\n# Explore the data frame:\n\ndim(dataset2)\n\nhead(dataset2)\n\nsummary(dataset2)\n\n# Convert date back to a date object:\n\ndate = as.Date(dataset2$date, '%Y-%m-%d')\n\nclass(date)\n\nhead(date)\n\n# All looks good, reassign the column:\n\ndataset = dataset2\n\ndataset$date = date\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE HOW THE DATA WERE MODIFIED!\n\n#-------------------------------------------------------------------------------*\n# ---- WRITE OUTPUT DATA FRAMES  ----\n#===============================================================================*\n\n# Take a final look at the dataset:\n\nhead(dataset)\n\nsummary (dataset)\n\n# If everything is looks okay we're ready to write formatted data frame:\n\nwrite.csv(dataset, \"data/formatted_datasets/dataset_223.csv\", row.names = F)\n\n# !GIT-ADD-COMMIT-PUSH THE FORMATTED DATASET IN THE DATA FILE, THEN GIT-ADD-\n# COMMIT-PUSH THE UPDATED DATA FOLDER!\n\n\n################################################################################*\n# ---- END CREATION OF FORMATTED DATA FRAME ----\n################################################################################*\n# If you had to set the formatting script aside and opened this in a new \n# session, source the script and load required libraries and dataset:\n\nlibrary(stringr)\nlibrary(plyr)\n\nsource('scripts/R-scripts/core-transient_functions.R')\n\ndataset = read.csv(\"data/formatted_datasets/dataset_223.csv\")\n\n#===============================================================================*\n# ---- MAKE PROPORTIONAL OCCUPANCY AND DATA SUMMARY FRAMES ----\n#===============================================================================*\n# We have now formatted the dataset to the finest possible spatial and temporal\n# grain, removed bad species, and added the dataset ID. It's now to make some\n# scale decisions and determine the proportional occupancies.\n\n#-------------------------------------------------------------------------------*\n# ---- TIME DATA ----\n#===============================================================================*\n# We start by extracting year from the dataset. Year will now be our DEFAULT\n# temporal grain. Decisions for finer temporal grains may be decided at a \n# later date.\n\n# Change date column to year:\n\ndataset$date = getYear(dataset$date)\n\n# Change column name:\n\nnames(dataset)[3] = 'year'\n\n#-------------------------------------------------------------------------------*\n# ---- SITE DATA ----\n#===============================================================================*\n# What is the appropriate sampling grain for sites? Return to the metadata to\n# see if there's any clues.\n\n# How many sites are there?\n\nlength(unique(dataset$site))\n\n# How many time and species records are there per site?\n\nsiteTable = ddply(dataset, .(site), summarize,\n                  nYear = length(unique(year)),\n                  nSp = length(unique(species)))\n\nhead(siteTable)\n\nsummary(siteTable)\n\n# We see that each of the sites was sampled with equivalent, and adequate,\n# time samples (>4) but that at least some sites have species richness \n# below the cut-off value of 10. Perhaps too many sites of with low sr?\n\n# Let's sort and have a look at the first few rows:\n\nhead(siteTable[order(siteTable$nSp),],20)\n\n# All 1's! How many sites have less than 10 species?\n\nnrow(siteTable)\nnrow(subset(siteTable, nSp < 10))\n\n# That's almost a third of the sites! This is a clue that the \n# smallest spatial sampling grain (quadrat) is too fine.\n\n# Let's try concatenating all but the quad field and explore the output. \n# We start by splitting site:\n\nsite = read.table(text = as.character(dataset$site), sep ='_')\n\nhead(site)\n\nsite1 = do.call('paste', c(site[,1:4],sep = '_'))\n\nhead(site1)\n\nlength(site1)\n\n# How have we changed the number of sites?\n\nlength(unique(dataset$site))\n\nlength(unique(site1))\n\n# We've reduced the number of sites to 28! How does the richness look\n# for this new spatial sampling grain?\n\ndataset1 = dataset\n\ndataset1$site = site1\n\nsiteTable = ddply(dataset1, .(site), summarize,\n                  nYear = length(unique(year)),\n                  nSp = length(unique(species)))\n\nhead(siteTable)\n\nsummary(siteTable)\n\nhead(siteTable[order(siteTable$nSp),],10)\n\n# For all but the first site (and perhaps the second), the species richness\n# is adequate.Change the dataset site column to this one:\n\ndataset$site = dataset1$site\n\n# Now let's remove the sites with inadequate sample sites:\n\nbadSites = subset(siteSummaryFun(dataset), spRich < 10 | nTime < 5)$site\n\ndataset1 = dataset[!dataset$site %in% badSites,]\n\n# Summarize the dataset to the new spatial grain:\n\ndataset2 = ddply(dataset1, .(datasetID, site, year, species), \n                 summarize, count = max(count))\n\nhead(dataset2)\n\ndim(dataset2)\n\nsummary(dataset2)\n\n# All looks good, rename dataset:\n\ndataset = dataset2\n\n# !GIT-ADD-COMMIT-PUSH AND DESCRIBE ANY SPATIAL GRAIN DECISIONS!\n\n# Note: In many instances, site definition will be spatially explicit (e.g., \n# lats and longs). When this is the case, we may need to summarize the data to\n# a courser precision (few decimal places). We can do so by using the \n# \"round_any\" function in Hadley Wickham's plyr package, specifying \"floor\" \n# as the rounding function.\n\n#-------------------------------------------------------------------------------*\n# ---- WRITE OUTPUT DATA FRAMES  ----\n#===============================================================================*\n\n# And make our proportional occurence data frame:\n\nwrite.csv(propOccFun(dataset), \"data/propOcc_datasets/propOcc_223.csv\", row.names = F)\n\n# !GIT-ADD-COMMIT-PUSH propOcc!\n\n# And make and write site summary dataset:\n\nwrite.csv(siteSummaryFun(dataset), 'data/siteSummaries/siteSummary_223.csv', row.names = F)\n\n# Note: Both the submodule and core-transient folder need to be pushed to, \n# in git bash:\n\n# cd data\n# git add formatted_datasets/dataset_208.csv\n# git commit -m \"added formatted dataset\"\n# git push\n# cd ..\n# git add data\n# git commit -m \"updated submodule with formatted dataset 208\"\n# git push\n\n#-------------------------------------------------------------------------------*\n# ---- EXPLORE YOUR DATASET SUMMARY INFO AND UPDATE THE DATA SOURCE TABLE  ----\n#===============================================================================*\n\n# !!!At this point, go to the data source table and provide:\n#   -central lat and lon (if available, if so, LatLonFLAG = 0, if you couldn't do\n#    it, add a flag of 1)\n#   -spatial_grain columns (T through W)\n#   -nRecs, nSites, nTime, nSpecies\n#   -temporal_grain columns (AH to AK)\n#   -Start and end year\n#   -Any necessary notes\n#   -flag any issues and put issue on github\n#   -git-add-commit-push data_source_table.csv\n\ndim(dataset)\n\nlength(unique(dataset$site))\n\nlength(unique(dataset$year))\n\nlength(unique(dataset$species))\n\n",
    "created" : 1425317642275.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "61704352",
    "id" : "3210B635",
    "lastKnownWriteTime" : 1425063225,
    "path" : "~/core-transient/scripts/R-scripts/data_cleaning_scripts/data_formatting_template.R",
    "project_path" : "scripts/R-scripts/data_cleaning_scripts/data_formatting_template.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}