---
title: "Logistic_function_trblsht"
author: "Molly Jenkins"
date: "March 5, 2018"
output: html_document
---
## Last attempted logistic function using TryCatch, nls, SSlogis from July

### Problems: 
For a quick project recap, my project is focusing on the proportion of core species in avian communities across a gradient of spatial scales using BBS data. It's a positive decelerating curve, and the data speak for themselves quite well. However! 

I am having a lot of trouble wrapping my head around the logistic function, its inputs, and what is being done with them. So a good brief overview in SSlogis and nls-type math would be great. I've tried to teach myself, but that has only gotten me so far!  

Our previous biggest issues were 1) getting the function to work at all and 2) trying to describe transient species. Since then I've been focusing on core species alone as the category of interest. I don't have to worry about trying to describe the transients here anymore, perhaps making for better outcomes. While the function technically works, it doesn't actually work in that I keep getting wacky parameter estimates for my Asymptote, xmid-inflexion point, and scale (slope at inflexion point) parameters. I revisit this in further detail below.   

We initially scrapped the logistic function after last Summer. Currently as an alternative, I extract five parameters from the scaling relationship for each stateroute, allowing those parameters to let the trends in the observed data speak for themselves. However, our curvature parameter is reliant on taking the differences between our observed values and the predicted values generated from a linear model. This isn't entirely appropriate given that our relationship is not linear. We aren't trying to say that it is at any point, we are mostly letting the data speak for themselves. But it would be really nice to have a fitted model and output to round out our analyses and help legitimize them to skeptical reviewers.  

![Our current 5 parameters extracted from observed data, panel C](/git/core-transient/output/plots/Molly_Plots/Figure2.png)

Here, inflexion point i takes the place of our midpoint/scale50 parameters (midpoint was scrapped for redundancy with scale50)

A stands for Asymptote and describes the point at which the relationship asymptotes out  

k is a scaling parameter and describes the slope of the relationship at inflexion point i 

Together these three parameters describe the relationship between core species and scale in a manner similar but different than our current approach. Our goal was to derive estimates for these three parameters for every focal route sampled, which I do - but the estimates seem poor. We would also like to use predicted values from the associated models to bootstrap R2 values for each focal route. Routes with poorer R2 values and/or parameters associated with a higher load of transients in the landscape may be explained by greater habitat heterogeneity at the site, which we planned to test by running an additional simple linear model comparing the r2 values and measures of habitat heterogeneity.  

For posterity, CA stands for "core-area" and CN stands for "core-abdundance". They are just simple coding shorthand labels for keeping track of my two alternate measures of scale. 



```{r setup, include = FALSE}

knitr::opts_knit$set(root.dir = "C:/git/core-transient")
#'#' Please download and install the following packages:
library(raster)
library(maps)
library(sp)
library(rgdal)
library(maptools)
library(rgeos)
library(dplyr)
library(fields)
library(tidyr)
library(ggplot2)
library(nlme)
library(gridExtra)
library(wesanderson)
library(stats)

# To run this script, you need temperature, precip, etc data, 
# which are currently stored in the following directories off of github: 

# Data directories
BBS = '//bioark.ad.unc.edu/HurlbertLab/Jenkins/BBS scaled/'

```



```{r}
bbs_allscales = read.csv("data/BBS/bbs_allscales.csv", header = TRUE)
coefs = read.csv("scripts/R-scripts/scale_analysis/intermed/coefs.csv", header = TRUE)
bbs_allscales = filter(bbs_allscales, focalrte %in% coefs$stateroute) #1003 focal rtes as should be

####Cross-scale analysis and visualization####
mod1 = lm(pctCore~logA, data = bbs_allscales) #explains ~72% of the variation in occ
mod2 = lm(pctCore~logN, data = bbs_allscales)
summary(mod1)

#compare to an NLS model 
mod3 = nls(pctCore ~ SSlogis(logA, Asym, xmid, scal), data = bbs_allscales)
mod4 = nls(pctCore ~ SSlogis(logN, Asym, xmid, scal), data = bbs_allscales)
summary(mod3)

plot(pctCore~logA, data = bbs_allscales, xlab = "Log Area" , ylab = "Proportion Core Species in Community")
plot(pctCore~logN, data = bbs_allscales, xlab = "Log Abundance" , ylab = "Proportion Core Species in Community")
#^^same pattern roughly; abundance describes ~same amt of variance as area so serves as a good proxy 

bbs_allscales$CApreds = predict(mod3, bbs_allscales)
bbs_allscales$CNpreds = predict(mod4, bbs_allscales) #gave me unique preds for all stateroute and scale combos so...
#use predicted values to bootstrap r2 overall but also individually for each route between observed and predicted 
#doing this separately from coef extraction otherwise will be too complicated! 


#these are for the whole thing tho, not each route individually 
CA_i = summary(mod3)$coefficients["xmid", "Estimate"] #in terms of the LOG AREA value...? since referring to x axis explicitly
CA_A = summary(mod3)$coefficients["Asym", "Estimate"] #in terms of the OCCUPANCY
CA_k = summary(mod3)$coefficients["scal", "Estimate"] #in terms of the OCCUPANCY 
```


## Extracting parms for individual stateroutes 

One of the major reasons we discared using the logistic models was because the estimates for the relationship seemed TOTALLY wacky - I'm embedding the output table generated from the below r code. It doesn't make sense at all for us to have any values
above 1 for our y axis or 5 for our x axis, unless there is something weird happening because we are running a logistic model 
on predictor data that have already been log-transformed. Would that explain our weird estimates? 

![note weirdly high values for A asymptote estimates](/git/core-transient/output/plots/Molly_Plots/troubleshooting_logistic.png)

```{r}
####Extract coefficients from scale-occupancy relationships for each route####
CA.df = data.frame(stateroute = numeric(), CA.A= numeric(), CA.i = numeric(), CA.k = numeric())
CN.df = data.frame(stateroute = numeric(), CN.A= numeric(), CN.i = numeric(), CN.k = numeric(), CN.r2 = numeric())

warnings = data.frame(stateroute = numeric(), warning = character())
stateroutes = unique(bbs_allscales$focalrte) #this stuff is the same, looks normal ^


#06/19 version of tryCatch
for(s in stateroutes){
  logsub = subset(bbs_allscales, bbs_allscales$focalrte == s)  
  #fitting the log curve for area (for each route)
  
 #CA - Core spp prop vs log area
  CAmodel = tryCatch({
    CAlog = nls(pctCore ~ SSlogis(logA, Asym, xmid, scal), data = logsub)
    # CApred = predict(CAlog)
    # CAlm.r2 = lm(logsub$pctCore ~ CApred)
    
    CA.i <- summary(CAlog)$coefficients["xmid","Estimate"]
    CA.A <- summary(CAlog)$coefficients["Asym","Estimate"]
    CA.k <- summary(CAlog)$coefficients["scal","Estimate"]
    # CA.r2 <- summary(CAlm.r2)$r.squared
    data.frame(stateroute = s, CA.A, CA.i, CA.k)
    
  }, warning = function(w) {
    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))
  }, error = function(e) {
    CA.i <- NA
    CA.A <- NA
    CA.k <- NA
    # CA.r2 <- NA
    temp = data.frame(stateroute = s, CA.A, CA.i, CA.k)
    return(temp)
    
  })
  CA.df = rbind(CA.df, CAmodel)
  
  #CN - prop core spp vs log abundance 
  CNmodel = tryCatch({
    CNlog = nls(pctCore ~ SSlogis(logN, Asym, xmid, scal), data = logsub)
    CNpred = predict(CNlog)
    # CNlm.r2 = lm(logsub$pctCore ~ CNpred) #bootstraping r2 vals for CNlog since not in summary stats
    
    CN.i <- summary(CNlog)$coefficients["xmid","Estimate"]
    CN.A <- summary(CAlog)$coefficients["Asym","Estimate"]
    CN.k <- summary(CNlog)$coefficients["scal","Estimate"]
    # CN.r2 <- summary(CNlm.r2)$r.squared
    data.frame(stateroute = s, CN.A, CN.i, CN.k)
    
  }, warning = function(w) {
    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))
  }, error = function(e) {
    CN.i <- NA
    CN.A <- NA
    CN.k <- NA
    # CN.r2 <- NA
    temp = data.frame(stateroute = s, CN.A, CN.i, CN.k)
    return(temp)
    
  })
  CN.df = rbind(CN.df, CNmodel)
  
}

#join all together using inner_join by focal rte, not cbind 
coefs_logis = CA.df %>% 
  inner_join(CN.df, CA.df, by = "stateroute") 
  

write.csv(coefs_logis, "C:/git/core-transient/scripts/R-scripts/scale_analysis/coefs_logis.csv", row.names = FALSE) #updated 03/05/2018
#still getting SUPER WEIRD values for coefs - like an asymtptote value of 20?? what? an i value of 16? 
```
