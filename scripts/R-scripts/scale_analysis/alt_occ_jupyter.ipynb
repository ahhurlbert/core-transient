{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'raster' was built under R version 3.3.3\"Loading required package: sp\n",
      "Warning message:\n",
      "\"package 'maps' was built under R version 3.3.3\"rgdal: version: 1.2-5, (SVN revision 648)\n",
      " Geospatial Data Abstraction Library extensions to R successfully loaded\n",
      " Loaded GDAL runtime: GDAL 2.0.1, released 2015/09/15\n",
      " Path to GDAL shared files: C:/Users/mollyfrn/Documents/R/win-library/3.3/rgdal/gdal\n",
      " Loaded PROJ.4 runtime: Rel. 4.9.2, 08 September 2015, [PJ_VERSION: 492]\n",
      " Path to PROJ.4 shared files: C:/Users/mollyfrn/Documents/R/win-library/3.3/rgdal/proj\n",
      " Linking to sp version: 1.2-4 \n",
      "Warning message:\n",
      "\"package 'maptools' was built under R version 3.3.3\"Checking rgeos availability: TRUE\n",
      "Warning message:\n",
      "\"package 'rgeos' was built under R version 3.3.3\"rgeos version: 0.3-22, (SVN revision 544)\n",
      " GEOS runtime version: 3.5.0-CAPI-1.9.0 r4084 \n",
      " Linking to sp version: 1.2-4 \n",
      " Polygon checking: TRUE \n",
      "\n",
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 3.3.3\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:rgeos':\n",
      "\n",
      "    intersect, setdiff, union\n",
      "\n",
      "The following objects are masked from 'package:raster':\n",
      "\n",
      "    intersect, select, union\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Warning message:\n",
      "\"package 'fields' was built under R version 3.3.3\"Loading required package: spam\n",
      "Loading required package: grid\n",
      "Spam version 1.4-0 (2016-08-29) is loaded.\n",
      "Type 'help( Spam)' or 'demo( spam)' for a short introduction \n",
      "and overview of this package.\n",
      "Help for individual functions is also obtained by adding the\n",
      "suffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n",
      "\n",
      "Attaching package: 'spam'\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    backsolve, forwardsolve\n",
      "\n",
      "Warning message:\n",
      "\"package 'tidyr' was built under R version 3.3.3\"\n",
      "Attaching package: 'tidyr'\n",
      "\n",
      "The following object is masked from 'package:raster':\n",
      "\n",
      "    extract\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'nlme' was built under R version 3.3.3\"\n",
      "Attaching package: 'nlme'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    collapse\n",
      "\n",
      "The following object is masked from 'package:raster':\n",
      "\n",
      "    getData\n",
      "\n",
      "Warning message:\n",
      "\"package 'gridExtra' was built under R version 3.3.3\"\n",
      "Attaching package: 'gridExtra'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#'Variation in occupancy at multiple scales WITHIN & ABOVE BBS sites\n",
    "#'REVISED ANALYSIS\n",
    "#'Molly F. Jenkins \n",
    "#'04/23/2017\n",
    "\n",
    "setwd(\"C:/git/core-transient\")\n",
    "#'#' Please download and install the following packages:\n",
    "library(raster)\n",
    "library(maps)\n",
    "library(sp)\n",
    "library(rgdal)\n",
    "library(maptools)\n",
    "library(rgeos)\n",
    "library(dplyr)\n",
    "library(fields)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(nlme)\n",
    "library(gridExtra)\n",
    "library(wesanderson)\n",
    "library(stats)\n",
    "\n",
    "\n",
    "# To run this script, you need temperature, precip, etc data, \n",
    "# which are currently stored in the following directories off of github: \n",
    "\n",
    "# Data directories\n",
    "tempdatadir = '//bioark.ad.unc.edu/HurlbertLab/GIS/ClimateData/BIOCLIM_meanTemp/'\n",
    "precipdata = '//bioark.ad.unc.edu/HurlbertLab/GIS/ClimateData/2-25-2011/prec/'\n",
    "ndvidata = \"//bioark.ad.unc.edu/HurlbertLab/GIS/MODIS NDVI/\"\n",
    "BBS = '//bioark.ad.unc.edu/HurlbertLab/Jenkins/BBS scaled/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'uniqrtes' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'uniqrtes' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "####Calculations for Occupancy above the scale of a BBS route####\n",
    "good_rtes2 = read.csv(paste(BBS, \"good_rtes2.csv\", sep = \"\"), header = TRUE) \n",
    "require(fields)\n",
    "#Distance calculation between all combination of routes to pair them by min dist for aggregation\n",
    "distances = rdist.earth(matrix(c(good_rtes2$Longi, good_rtes2$Lati), ncol=2),\n",
    "                        matrix(c(good_rtes2$Longi, good_rtes2$Lati), ncol=2),\n",
    "                        miles=FALSE, R=6371)\n",
    "dist.df = data.frame(rte1 = rep(good_rtes2$stateroute, each = nrow(good_rtes2)),\n",
    "                     rte2 = rep(good_rtes2$stateroute, times = nrow(good_rtes2)),\n",
    "                     dist = as.vector(distances))\n",
    "\n",
    "\n",
    "#bring in NON-50 stop data for above-route scale res\n",
    "bbs_allyears = read.csv(paste(BBS, \"bbs_allyears.csv\", sep = \"\"), header = TRUE)\n",
    "bbs_bestAous = bbs_allyears %>% \n",
    "  filter(Aou > 2880 & !(Aou >= 3650 & Aou <= 3810) & !(Aou >= 3900 & Aou <= 3910) & \n",
    "                 !(Aou >= 4160 & Aou <= 4210) & Aou != 7010)  #excluding shorebirds and owls, data less reliable\n",
    "\n",
    "\n",
    "numrtes = 1:65 # based on min common number in top 6 grid cells, see grid_sampling_justification script \n",
    "output = data.frame(r = NULL, nu = NULL, AOU = NULL, occ = NULL)\n",
    "for (r in uniqrtes) {\n",
    "  for (nu in numrtes) {\n",
    "  tmp = filter(dist.df2, rte1 == r) %>%\n",
    "    arrange(dist)\n",
    "  tmprtes = tmp$rte2[1:nu]   \n",
    "  #Aggregate routes together based on distance, calc occupancy, etc\n",
    "  \n",
    "  bbssub = filter(bbs_bestAous, stateroute %in% c(r, tmprtes)) \n",
    "  bbsuniq = unique(bbssub[, c('Aou', 'Year')])\n",
    "  occs = bbsuniq %>% dplyr::count(Aou) %>% dplyr::mutate(occ = n/15)\n",
    "  \n",
    "  temp = data.frame(focalrte = r,\n",
    "                    numrtes = nu+1,                           #total # routes being aggregated\n",
    "                    meanOcc = mean(occs$occ, na.rm =T),       #mean occupancy\n",
    "                    pctCore = sum(occs$occ > 2/3)/nrow(occs),\n",
    "                    pctTrans = sum(occs$occ <= 1/3)/nrow(occs), #fraction of species that are transient\n",
    "                    totalAbun = sum(bbssub$SpeciesTotal)/15,  #total community size (per year)\n",
    "                    maxRadius = tmp$dist[nu])                 #radius including rtes aggregated\n",
    "  output = rbind(output, temp)\n",
    "  print(paste(\"Focal rte\", r, \"#' rtes sampled\", nu))\n",
    "  \n",
    "  } #n loop\n",
    "  \n",
    "} #r loop\n",
    "\n",
    "bbs_focal_occs = as.data.frame(output)\n",
    "#Calc area for above route scale\n",
    "bbs_focal_occs$area = bbs_focal_occs$numrtes*50*(pi*(0.4^2)) #number of routes * fifty stops * area in sq km of a stop \n",
    "# write.csv(bbs_focal_occs, \"/scripts/R-scripts/scale_analysis/bbs_focal_occs.csv\", row.names = FALSE)\n",
    "\n",
    "\n",
    "####Plotting BBS occupancy at scales above a BBS route####\n",
    "plot(bbs_focal_occs$numrtes, bbs_focal_occs$meanOcc, xlab = \"#' routes\", ylab = \"mean occupancy\")\n",
    "par(mfrow = c(2, 1))\n",
    "plot(bbs_focal_occs$numrtes, bbs_focal_occs$pctTran, xlab = \"#' routes\", ylab = \"% Trans\")\n",
    "plot(bbs_focal_occs$numrtes, bbs_focal_occs$pctCore, xlab = \"#' routes\", ylab = \"% Core\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####Below-route occupancy calculations####\n",
    "fifty_allyears = read.csv(paste(BBS, \"fifty_allyears.csv\", sep = \"\"), header = TRUE)\n",
    "fifty_bestAous = fifty_allyears %>% \n",
    "  filter(AOU > 2880 & !(AOU >= 3650 & AOU <= 3810) & !(AOU >= 3900 & AOU <= 3910) & \n",
    "           !(AOU >= 4160 & AOU <= 4210) & AOU != 7010) #leaving out owls, waterbirds as less reliable data\n",
    "\n",
    "#occ_counts function for calculating occupancy at any scale\n",
    "occ_counts = function(countData, countColumns, scale) {\n",
    "  bbssub = countData[, c(\"stateroute\", \"year\", \"AOU\", countColumns)]\n",
    "  bbssub$groupCount = rowSums(bbssub[, countColumns])\n",
    "  bbsu = unique(bbssub[bbssub[, \"groupCount\"]!= 0, c(\"stateroute\", \"year\", \"AOU\")]) \n",
    "  \n",
    "  abun.summ = bbssub %>% #abundance\n",
    "    group_by(stateroute, year) %>%  \n",
    "    summarize(totalN = sum(groupCount)) %>%\n",
    "    group_by(stateroute) %>%\n",
    "    summarize(aveN = mean(totalN))\n",
    "    \n",
    "  occ.summ = bbsu %>% #occupancy\n",
    "    count(stateroute, AOU) %>%\n",
    "    mutate(occ = n/15, scale = scale, subrouteID = countColumns[1]) %>%\n",
    "    group_by(stateroute) %>%\n",
    "    summarize(meanOcc = mean(occ), \n",
    "              pctCore = sum(occ > 2/3)/length(occ),\n",
    "              pctTran = sum(occ <= 1/3)/length(occ)) %>%\n",
    "              #spRichTrans33  \n",
    "               # spRichTrans25 = sum(occ <= 1/4)/length(occ),\n",
    "              # spRichTrans10 = sum(occ <= 0.1)/length(occ)) %>%\n",
    "    mutate(scale = paste(scale, g, sep = \"-\")) %>%\n",
    "    left_join(abun.summ, by = 'stateroute')\n",
    "    return(occ.summ)\n",
    "}\n",
    "\n",
    "\n",
    "# Generic calculation of occupancy for a specified scale\n",
    "scales = c(5, 10, 25, 50)\n",
    "\n",
    "output = c()\n",
    "for (scale in scales) {\n",
    "  numGroups = floor(50/scale)\n",
    "  for (g in 1:numGroups) {\n",
    "    groupedCols = paste(\"Stop\", ((g-1)*scale + 1):(g*scale), sep = \"\")\n",
    "    temp = occ_counts(fifty_bestAous, groupedCols, scale)\n",
    "    output = rbind(output, temp) \n",
    "  }\n",
    "}\n",
    "bbs_below<-data.frame(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####Binding above and below route scales, calc area####\n",
    "bbs_focal_occs = read.csv(paste(BBS, \"bbs_focal_occs.csv\", sep = \"\"), header = TRUE) \n",
    "bbs_below = read.csv(paste(BBS, \"bbs_below.csv\", sep = \"\"), header = T)\n",
    "\n",
    "\n",
    "#adding maxRadius column to bbs_below w/NA's + renaming and rearranging columns accordingly, creating area cols\n",
    "bbs_below= bbs_below %>% \n",
    "  mutate(maxRadius = c(\"NA\")) %>%\n",
    "  dplyr::rename(focalrte = stateroute) %>%\n",
    "  select(focalrte, scale, everything()) %>%\n",
    "  mutate(area = (as.integer(lapply(strsplit(as.character(bbs_below$scale), \n",
    "                                           split=\"-\"), \"[\", 1)))*(pi*(0.4^2))) \n",
    "#modify and split scale so that it's just the # of stops in each seg; not the seg order # preceded by a \"-\"\n",
    "\n",
    "\n",
    "bbs_focal_occs = bbs_focal_occs %>% \n",
    "  dplyr::rename(scale = numrtes, aveN = totalAbun) %>%\n",
    "  mutate(area = scale*50*(pi*(0.4^2))) #area in km by # of routes * 50 stops in each rte * area of a stop (for above-route scale later)\n",
    "bbs_focal_occs$scale = as.factor(bbs_focal_occs$scale)\n",
    "\n",
    "\n",
    "bbs_allscales = rbind(bbs_below, bbs_focal_occs) #rbind ok since all share column names\n",
    "#write.csv(bbs_allscales, \"C:/git/core-transient/scripts/R-scripts/scale_analysis/bbs_allscales.csv\", row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "\"cannot open file 'data/bbs_allscales.csv': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.csv(\"data/bbs_allscales.csv\", header = TRUE)",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "\n",
    "####Cross-scale analysis and visualization####\n",
    "bbs_allscales = read.csv(\"data/bbs_allscales.csv\", header = TRUE)\n",
    "bbs_allscales$logA = log10(bbs_allscales$area)\n",
    "bbs_allscales$logN = log10(bbs_allscales$aveN)\n",
    "bbs_allscales$lnA = log(bbs_allscales$area) #log is the natural log \n",
    "bbs_allscales$lnN = log(bbs_allscales$aveN) #rerun plots with this?\n",
    "\n",
    "mod1 = lm(meanOcc~logA, data = bbs_allscales) #explains ~50% of the variation in occ\n",
    "mod2 = lm(meanOcc~logN, data = bbs_allscales)\n",
    "summary(mod1)\n",
    "\n",
    "plot(meanOcc~logA, data = bbs_allscales, xlab = \"Log Area\" , ylab = \"Mean Temporal Occupancy\")\n",
    "plot(meanOcc~logN, data = bbs_allscales, xlab = \"Average Abundance\" , ylab = \"Mean Temporal Occupancy\")\n",
    "#^^same pattern roughly; abundance describes ~same amt of variance as area so serves as a good proxy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Extract coefficients from scale-occupancy relationships for analysis####\n",
    "OA.df = data.frame(stateroute = numeric(), OA.A= numeric(), OA.i = numeric(), OA.k = numeric(), OA.r2 = numeric())\n",
    "ON.df = data.frame(stateroute = numeric(), ON.A= numeric(), ON.i = numeric(), ON.k = numeric(), ON.r2 = numeric())\n",
    "CA.df = data.frame(stateroute = numeric(), CA.A= numeric(), CA.i = numeric(), CA.k = numeric(), CA.r2 = numeric())\n",
    "CN.df = data.frame(stateroute = numeric(), CN.A= numeric(), CN.i = numeric(), CN.k = numeric(), CN.r2 = numeric())\n",
    "TA.df = data.frame(stateroute = numeric(), TAexp= numeric(), TApow = numeric(), TAexp.r2 = numeric(), TApow.r2 = numeric())\n",
    "TN.df = data.frame(stateroute = numeric(), TNexp= numeric(), TNpow = numeric(), TNexp.r2 = numeric(), TNpow.r2 = numeric())\n",
    "\n",
    "warnings = data.frame(stateroute = numeric(), warning = character())\n",
    "stateroutes = unique(bbs_allscales$focalrte)\n",
    "\n",
    "for(s in stateroutes){\n",
    "  logsub = subset(bbs_allscales, bbs_allscales$focalrte == s)  \n",
    "  #OA \n",
    "  OAmodel = tryCatch({\n",
    "    OAlog = nls(meanOcc ~ SSlogis(logA, Asym, xmid, scal), data = logsub)\n",
    "    OApred = predict(OAlog)\n",
    "    OAlm.r2 = lm(logsub$meanOcc ~ OApred)\n",
    "    return(data.frame(stateroute = s, OA.A, OA.i, OA.k, \n",
    "                      OA.r2 = summary(OAlm.r2)$r.squared))\n",
    "    return(data.frame(stateroute = s, OA.pred = OApred)) #can I return multiple outputs from a trycatch?\n",
    "  }, warning = function(w) {\n",
    "    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))\n",
    "  }, error = function(e) {\n",
    "    OA.i <- NA\n",
    "    OA.A <- NA\n",
    "    OA.k <- NA\n",
    "    OA.r2 <- NA\n",
    "    OA.pred <- NA\n",
    "  }, finally = {\n",
    "    OA.i <- summary(OAlog)$coefficients[\"xmid\",\"Estimate\"]\n",
    "    OA.A <- summary(OAlog)$coefficients[\"Asym\",\"Estimate\"]\n",
    "    OA.k <- summary(OAlog)$coefficients[\"scal\",\"Estimate\"]\n",
    "    OA.r2 <- summary(OAlm.r2)$r.squared\n",
    "    OA.pred <- OApred\n",
    "    })\n",
    "  OA.temp = data.frame(stateroute = s, OA.A, OA.i, OA.k, OA.r2)\n",
    "  OA.temppred = data.frame(stateroute = s, OA.pred = OA.pred)\n",
    "  OA.df = rbind(OA.df, OA.temp)\n",
    "  OA.pred.df = rbind(OA.pred.df, OA.temppred)\n",
    "  \n",
    "  #ON \n",
    "  ONmodel = tryCatch({\n",
    "    ONlog = nls(meanOcc ~ SSlogis(logN, Asym, xmid, scal), data = logsub)\n",
    "    ONpred = predict(ONlog)\n",
    "    ONlm.r2 = lm(logsub$meanOcc ~ ONpred)\n",
    "    return(data.frame(stateroute = s, ON.A, ON.i, ON.k, \n",
    "                      ON.r2 = summary(ONlm.r2)$r.squared, ON.pred = ONpred))\n",
    "  }, warning = function(w) {\n",
    "    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))\n",
    "  }, error = function(e) {\n",
    "    ON.i <- NA\n",
    "    ON.A <- NA\n",
    "    ON.k <- NA\n",
    "    ON.r2 <- NA\n",
    "    ON.pred <- NA\n",
    "  }, finally = {\n",
    "    ON.i <- summary(ONlog)$coefficients[\"xmid\",\"Estimate\"]\n",
    "    ON.A <- summary(ONlog)$coefficients[\"Asym\",\"Estimate\"]\n",
    "    ON.k <- summary(ONlog)$coefficients[\"scal\",\"Estimate\"]\n",
    "    ON.r2 <- summary(ONlm.r2)$r.squared\n",
    "    ON.pred <- ONpred\n",
    "    })\n",
    "  ON.temp = data.frame(stateroute = s, ON.A, ON.i, ON.k, ON.r2, ON.pred) #fix\n",
    "  ON.df = rbind(ON.df, ON.temp)\n",
    "  \n",
    "  #CA\n",
    "  CAmodel = tryCatch({\n",
    "    CAlog = nls(pctCore ~ SSlogis(logA, Asym, xmid, scal), data = logsub)\n",
    "    CApred = predict(CAlog)\n",
    "    CAlm.r2 = lm(logsub$pctCore ~ CApred)\n",
    "    return(data.frame(stateroute = s, CA.A, CA.i, CA.k, \n",
    "                      CA.r2 = summary(CAlm.r2)$r.squared, CA.pred = CApred))\n",
    "  }, warning = function(w) {\n",
    "    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))\n",
    "  }, error = function(e) {\n",
    "    CA.i <- NA\n",
    "    CA.A <- NA\n",
    "    CA.k <- NA\n",
    "    CA.r2 <- NA\n",
    "    CA.pred <- NA\n",
    "  }, finally = {\n",
    "    CA.i <- summary(CAlog)$coefficients[\"xmid\",\"Estimate\"]\n",
    "    CA.A <- summary(CAlog)$coefficients[\"Asym\",\"Estimate\"]\n",
    "    CA.k <- summary(CAlog)$coefficients[\"scal\",\"Estimate\"]\n",
    "    CA.r2 = summary(CAlm.r2)$r.squared\n",
    "    CA.pred <- CApred\n",
    "  })\n",
    "  CA.temp = data.frame(stateroute = s, CA.A, CA.i, CA.k, CA.r2, CA.pred) \n",
    "  CA.df = rbind(CA.df, CA.temp)\n",
    "  \n",
    "  #CN\n",
    "  CNmodel = tryCatch({\n",
    "    CNlog = nls(pctCore ~ SSlogis(logN, Asym, xmid, scal), data = logsub)\n",
    "    CNpred = predict(CNlog)\n",
    "    CNlm.r2 = lm(logsub$pctCore ~ CNpred) #bootstraping r2 vals for CNlog since not in summary stats\n",
    "    return(data.frame(stateroute = s, CN.A, CN.i, CN.k, \n",
    "                      CN.r2 = summary(CNlm.r2)$r.squared, CN.pred = CNpred))\n",
    "  }, warning = function(w) {\n",
    "    warnings = rbind(warnings, data.frame(stateroute = s, warning = w))\n",
    "  }, error = function(e) {\n",
    "    CN.i <- NA\n",
    "    CN.A <- NA\n",
    "    CN.k <- NA\n",
    "    CN.r2 <- NA\n",
    "    CN.pred <- NA\n",
    "  }, finally = {\n",
    "    CN.i <- summary(CNlog)$coefficients[\"xmid\",\"Estimate\"]\n",
    "    CN.A <- summary(CNlog)$coefficients[\"Asym\",\"Estimate\"]\n",
    "    CN.k <- summary(CNlog)$coefficients[\"scal\",\"Estimate\"]\n",
    "    CN.r2 <- summary(CNlm.r2)$r.squared\n",
    "    CN.pred = CNpred\n",
    "    })\n",
    "  CN.temp = data.frame(stateroute = s, CN.A, CN.i, CN.k, CN.r2, CN.pred) \n",
    "  CN.df = rbind(CN.df, CN.temp)\n",
    "\n",
    "  # Fitting % transient\n",
    "  #TA #revisit!!\n",
    "  TAlog = lm(log(pctTran) ~ lnA, data = logsub) #try with log10(pctTran), log(pctTran) ~ logA, and pctTran ~ logA since relationships wonky  \n",
    "  TA = lm(log(pctTran) ~ area, data = logsub)\n",
    "  TA.temp = data.frame(stateroute = s, \n",
    "                       TAexp = TAlog$coefficients[2],\n",
    "                       TApow = TA$coefficients[2], \n",
    "                       TAexp.r2 = summary(TAlog)$r.squared, \n",
    "                       TApow.r2 = summary(TA)$r.squared) \n",
    "  TA.df = rbind(TA.df, TA.temp)\n",
    "  \n",
    "  #TN  \n",
    "  TNlog = lm(log(pctTran) ~ lnN, data = logsub)\n",
    "  TN = lm(log(pctTran) ~ area, data = logsub)\n",
    "    TN.temp = data.frame(stateroute = s, \n",
    "                       TNexp = TNlog$coefficients[2],\n",
    "                       TNpow = TN$coefficients[2], \n",
    "                       TNexp.r2 = summary(TNlog)$r.squared, \n",
    "                       TNpow.r2 = summary(TN)$r.squared)\n",
    "  TN.df = rbind(TN.df, TN.temp)\n",
    "}\n",
    "\n",
    "#join all together using inner_join by focal rte, not cbind \n",
    "coefs = OA.df %>% \n",
    "  inner_join(ON.df, OA.df, by = \"stateroute\") %>% \n",
    "  inner_join(CA.df, OA.df, by = \"stateroute\") %>% \n",
    "  inner_join(CN.df, OA.df, by = \"stateroute\") %>% \n",
    "  inner_join(TA.df, OA.df, by = \"stateroute\") %>% \n",
    "  inner_join(TN.df, OA.df, by = \"stateroute\")  \n",
    "\n",
    "#write.csv(coefs, \"C:/git/core-transient/scripts/R-scripts/scale_analysis/coefs.csv\", row.names = FALSE) #updated 02/27\n",
    "#exp mods have much better r2 vals for pctTran than power \n",
    "\n",
    "\n",
    "####Plotting occupancy-scale relationships with observed and predicted values####\n",
    "bbs_allscales = read.csv(\"data/bbs_allscales.csv\", header = TRUE)\n",
    "bbs_allscales$logA = log10(bbs_allscales$area)\n",
    "bbs_allscales$logN = log10(bbs_allscales$aveN)\n",
    "bbs_allscales$lnA = log(bbs_allscales$area) #log is the natural log \n",
    "bbs_allscales$lnN = log(bbs_allscales$aveN) #rerun plots with this?\n",
    "coefs = read.csv(\"scripts/R-scripts/scale_analysis/coefs.csv\", header = TRUE)\n",
    "\n",
    "#function for extracting predicted values from models built with observed data\n",
    "logistic_fcn = function(x, Asym, xmid, scal) {\n",
    "  out = Asym/(1 + exp((xmid - x)/scal))\n",
    "  return(out)\n",
    "}\n",
    "\n",
    "\n",
    "preds.df = data.frame(stateroute = numeric(), logA = numeric(), \n",
    "                      OApreds= numeric(), ONpreds = numeric(), \n",
    "                      CApreds = numeric(), CNpreds = numeric(),\n",
    "                      TApreds = numeric(), TNpreds = numeric())\n",
    "\n",
    "\n",
    "pdf(\"output/plots/Molly Plots/BBS_scaleplots.pdf\", onefile = TRUE)\n",
    "coef_join = coefs %>% inner_join(bbs_allscales, by = c(\"stateroute\"=\"focalrte\"))\n",
    "stateroutes = unique(bbs_allscales$focalrte)\n",
    "\n",
    "#extracting predicted values and plotting in same loop\n",
    "for (s in stateroutes) {\n",
    "  theme_set(theme_bw()+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()))\n",
    "  coef_sub = subset(coef_join, coef_join$stateroute == s)\n",
    "  logA = coef_sub$logA\n",
    "  \n",
    "  #OA\n",
    "  OApreds = logistic_fcn(coef_sub[,33], coef_sub[,2], coef_sub[,3], coef_sub[,4]) \n",
    "   plot1 = ggplot(coef_sub, aes(x = logA, y = meanOcc))+geom_point(colour = \"firebrick\")+\n",
    "     geom_line(aes(x = logA, y = OApreds), color = \"navy\") +labs(x = \"Log area\", y = \"Mean % Occupancy\")\n",
    "  \n",
    "  \n",
    "  #ON\n",
    "  ONpreds = logistic_fcn(coef_sub[,34], coef_sub[,6], coef_sub[,7], coef_sub[,8])\n",
    "   plot2 = ggplot(coef_sub, aes(x = logN, y = meanOcc))+geom_point(colour = \"firebrick\")+\n",
    "     geom_line(aes(x = logN, y = ONpreds), color = \"navy\") +labs(x = \"Log abundance\", y = \"Mean % Occupancy\")\n",
    "   \n",
    " \n",
    "  #CA\n",
    "  CApreds = logistic_fcn(coef_sub[,33], coef_sub[,10], coef_sub[,11], coef_sub[,12])\n",
    "   plot1_2= ggplot(coef_sub, aes(x = logA, y = pctCore))+geom_point(colour = \"turquoise\")+\n",
    "     geom_line(aes(x = logA, y = CApreds), color = \"navy\")+labs(x = \"Log area\", y = \"% Core Occupancy\")\n",
    "   \n",
    "  #aveN\n",
    "  #CN\n",
    "  CNpreds = logistic_fcn(coef_sub[,34], coef_sub[,14], coef_sub[,15], coef_sub[,16])\n",
    "   plot2_2= ggplot(coef_sub, aes(x = logN, y = pctCore))+geom_point(colour = \"turquoise\")+\n",
    "     geom_line(aes(x = logN, y = CNpreds), color = \"navy\")+labs(x = \"Log abundance\", y = \"% Core Occupancy\")\n",
    "\n",
    "  #using exponential function since higher explanatory power than pwr function\n",
    "  #TA\n",
    "  TApreds =  coef_sub[,35]*(coef_sub[,18]) #35 = optimum; replacing ^ with * bc natural log, removing -1\n",
    "  plot1_3 = ggplot(coef_sub, aes(x = lnA, y = log(pctTran)))+geom_point(colour = \"olivedrab\")+\n",
    "    geom_line(aes(x = lnA, y = log(TApreds)), color = \"navy\") +labs(x = \"Log area\", y = \"% Transient Occupancy\")\n",
    "\n",
    "  #TN\n",
    "  TNpreds = coef_sub[,36]*(coef_sub[,22])\n",
    "  plot2_3 = ggplot(coef_sub, aes(x = lnN, y = log(pctTran)))+geom_point(colour = \"olivedrab\")+\n",
    "    geom_line(aes(x = lnN, y = TNpreds), color = \"navy\")+labs(x = \"Log abundance\", y = \"% Transient Occupancy\")\n",
    "\n",
    "  #storing plots\n",
    "  predplot = grid.arrange(plot1, plot2, plot1_2, plot2_2, plot1_3, plot2_3,\n",
    "                           ncol=2, top = paste(\"predplot_\", s, sep = \"\"))\n",
    "  #storing preds:\n",
    "  temp.df = data.frame(stateroute = s, logA = logA, \n",
    "                       OApreds= OApreds , ONpreds = ONpreds, \n",
    "                       CApreds = CApreds, CNpreds = CNpreds,\n",
    "                       TApreds = TApreds, TNpreds = TNpreds)\n",
    "  preds.df = rbind(preds.df, temp.df)\n",
    "  \n",
    "}\n",
    "dev.off()\n",
    "write.csv(preds.df, \"C:/git/core-transient/scripts/R-scripts/scale_analysis/preds.csv\", row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####Env data add-in####\n",
    "bbs_allscales = read.csv(\"data/BBS/bbs_allscales.csv\", header = TRUE)\n",
    "bbs_latlon = read.csv(paste(BBS, \"good_rtes2.csv\", sep = \"\"), header = TRUE)\n",
    "bbs_allscales = dplyr::rename(bbs_latlon, focalrte = stateroute) %>%\n",
    "  right_join(bbs_allscales, by = \"focalrte\")\n",
    "sites = data.frame(longitude = bbs_latlon$Longi, latitude = bbs_latlon$Lati) \n",
    "points(sites$longitude, sites$latitude, col= \"red\", pch=16) #check on map\n",
    "\n",
    "#temp\n",
    "precip_temp = raster::getData(\"worldclim\", var = \"bio\", res = 2.5) #supposed to be already /10 according to site\n",
    "btest_temp = raster::extract(temp, sites[1:5,]) #just want bio1 -> mean annual temp \n",
    "#trying within latlon because 2001 test ran fine; keeping stateroute ID's associated\n",
    "bbs_latlon$vartemp = raster::extract(temp, sites, buffer = 40000, fun = var)\n",
    "#getData working for all but insanely slow to extract\n",
    "\n",
    "#precip \n",
    "prec = raster::getData(\"worldclim\", var = \"bio\", res = 2.5) #just want bio12 -> annual precip \n",
    "b_test_meanP = raster::extract(prec, sites[1:5,], buffer = 40000, fun = mean)\n",
    "bbs_latlon$varP = raster::extract(prec, sites, buffer = 40000, fun = var)\n",
    "\n",
    "#ndvi \n",
    "ndvi = raster(paste(ndvidata, \"Vegetation_Indices_may-aug_2000-2010.gri\", sep = \"\")) #can't find on getData\n",
    "#coming out as a RasterLayer, converting to rasterstack \n",
    "ndvis = stack(ndvi)\n",
    "\n",
    "ndvimean = ndvi/10000\n",
    "btest_ndvi = raster::extract(ndvis, sites[1:5,], buffer = 40000, fun = mean)\n",
    "bbs_latlon$varndvi = raster::extract(ndvimean, sites, buffer = 40000, fun = var)\n",
    "\n",
    "#elev \n",
    "elev = raster::getData(\"worldclim\", var = \"alt\", res = 2.5) #raster::getData(\"alt\", country = 'USA', res = 2.5)\n",
    "#coming out as a RasterLayer, converting to rasterstack \n",
    "elevs = stack(elev)\n",
    "\n",
    "btest_elev = raster::extract(elevs, sites[1:5,], buffer = 40000, fun = mean)\n",
    "bbs_latlon$varelev = raster::extract(elevs, sites, buffer = 40000, fun = var) \n",
    "\n",
    "bbs_envs = bbs_latlon \n",
    "#write.csv(bbs_envs, \"scripts/R-scripts/scale_analysis/bbs_envs.csv\", row.names = FALSE) \n",
    "#wrote file 03/28 w/elev from getData and using old env data; need to overwrite with updated env vars\n",
    "\n",
    "#merge into bbs_allscales data\n",
    "\n",
    "####Alternate 'cookie-cutter' method for env variables####\n",
    "#based on Sara's code from summary_and_analysis.R file\n",
    "\n",
    "# Makes routes into a spatialPointsDataframe\n",
    "coordinates(all_latlongs)=c('Lon','Lat')\n",
    "projection(all_latlongs) = CRS(\"+proj=longlat +ellps=WGS84\")\n",
    "prj.string <- \"+proj=laea +lat_0=45.235 +lon_0=-106.675 +units=km\"\n",
    "# Transforms routes to an equal-area projection - see previously defined prj.string\n",
    "routes.laea = spTransform(all_latlongs, CRS(prj.string))\n",
    "\n",
    "##### extracting elevation data ####\n",
    "# A function that draws a circle of radius r around a point: p (x,y)\n",
    "RADIUS = 40\n",
    "\n",
    "make.cir = function(p,r){\n",
    "  points=c()\n",
    "  for(i in 1:360){\n",
    "    theta = i*2*pi/360\n",
    "    y = p[2] + r*cos(theta)\n",
    "    x = p[1] + r*sin(theta)\n",
    "    points = rbind(points,c(x,y))\n",
    "  }\n",
    "  points=rbind(points,points[1,])\n",
    "  circle=Polygon(points,hole=F)\n",
    "  circle\n",
    "}\n",
    "\n",
    "routes.laea@data$dId_site = paste(routes.laea@data$datasetID, routes.laea@data$site, sep = \"_\")\n",
    "routes.laea@data$unique = 1:16602\n",
    "\n",
    "\n",
    "#Draw circles around all routes \n",
    "circs = sapply(1:nrow(routes.laea@data), function(x){\n",
    "  circ =  make.cir(routes.laea@coords[x,],RADIUS)\n",
    "  circ = Polygons(list(circ),ID=routes.laea$unique[x]) \n",
    "}\n",
    ")\n",
    "\n",
    "circs.sp = SpatialPolygons(circs, proj4string=CRS(prj.string))\n",
    "\n",
    "# Check that circle locations look right\n",
    "plot(circs.sp)\n",
    "\n",
    "# read in elevation raster at 1 km resolution\n",
    "elev <- raster(\"Z:/GIS/DEM/sdat_10003_1_20170424_102000103.tif\")\n",
    "NorthAm = readOGR(\"Z:/GIS/geography\", \"continent\")\n",
    "\n",
    "plot(elev)\n",
    "plot(NorthAm,add=TRUE)\n",
    "\n",
    "elevNA <- raster::mask(elev, NorthAm)\n",
    "\n",
    "\n",
    "elev.point = raster::extract(elevNA, routes.laea)\n",
    "elev.mean = raster::extract(elevNA, circs.sp, fun = mean, na.rm=T)\n",
    "elev.var = raster::extract(elevNA, circs.sp, fun = var, na.rm=T)\n",
    "\n",
    "env_elev = data.frame(unique = routes.laea@data$unique, elev.point = elev.point, elev.mean = elev.mean, elev.var = elev.var)\n",
    "\n",
    "\n",
    "lat_scale_elev = merge(routes.laea, env_elev, by = c(\"unique\")) # checked to make sure order lined up, d/n seem to be another way to merge since DID keeps getting lost\n",
    "lat_scale_elev = data.frame(lat_scale_elev)\n",
    "\n",
    "\n",
    "####Coef vs env variation models####\n",
    "bbs_envs = read.csv(\"scripts/R-scripts/scale_analysis/bbs_envs.csv\", header = TRUE)\n",
    "coefs = read.csv(\"scripts/R-scripts/scale_analysis/coefs.csv\", header = TRUE)\n",
    "uniq_env = unique(bbs_envs[, c('focalrte', 'temp', 'vartemp', 'meanP', 'varP', 'ndvi', 'varndvi', 'elev')])\n",
    "env_coefs = inner_join(coefs, uniq_env, by = c('stateroute' = 'focalrte'))\n",
    "covmatrix = round(cor(coefs[, 2:ncol(coefs)]), 2)\n",
    "\n",
    "\n",
    "# nested loop for examining variation in coefs/fitted curves explained by env vars \n",
    "rsqrd_df = data.frame(dep = character(), ind = character(), r2 = numeric())\n",
    "\n",
    "for (d in 2:25) {\n",
    "  for (i in 26:ncol(env_coefs)) {\n",
    "    tempmod = lm(env_coefs[,d] ~ env_coefs[,i])\n",
    "    tempdf = data.frame(dep = names(env_coefs)[d], \n",
    "                        ind = names(env_coefs)[i], \n",
    "                        r2 = summary(tempmod)$r.squared)\n",
    "    rsqrd_df = rbind(rsqrd_df, tempdf)\n",
    "  }\n",
    "}\n",
    "#write.csv(rsqrd_df, \"scripts/R-scripts/scale_analysis/mod_rsqrds.csv\", row.names = FALSE) #updated 03/28 with elev\n",
    "\n",
    "\n",
    "####Visually Characterizing r2 vals####\n",
    "rsqrd_df = read.csv(\"scripts/R-scripts/scale_analysis/mod_rsqrds.csv\", header = TRUE)\n",
    "\n",
    "ggplot(data = rsqrd_df, aes(x = ind, y = r2, fill = ind))+geom_boxplot()+theme_classic()+\n",
    "  scale_fill_manual(values = wes_palette(\"BottleRocket\"))+theme(legend.position=\"none\")+\n",
    "  labs(x = \"Environmental variables\", y = \"Variation Explained (R^2)\")\n",
    "\n",
    "#excluding transient data for incompleteness\n",
    "rsub_i = rsqrd_df %>%\n",
    "  filter(dep == \"OA.i\" | dep == \"ON.i\" | dep == \"CA.i\" | dep == \"CN.i\") %>%\n",
    "  filter(ind == \"elev\" | ind == \"meanP\" | ind == \"ndvi\" | ind == \"temp\")\n",
    "rsub_i = droplevels(rsub_i) #removing ghost levels to ensure correct plotting/analyses\n",
    "\n",
    "ggplot(data = rsub_i, aes(x = ind, y = r2)) + geom_boxplot()+theme_classic() #what I used for poster w/out color \n",
    "\n",
    "\n",
    "#separate analysis for just transients since relationship not immediately apparent\n",
    "rsub_t = rsqrd_df %>%\n",
    "  filter(dep == \"TAexp\" | dep == \"TApow\" | dep == \"TNexp\" | dep == \"TNpow\") %>%\n",
    "  filter(ind == \"elev\" | ind == \"meanP\" | ind == \"ndvi\" | ind == \"temp\")\n",
    "rsub_t = droplevels(rsub_t) #removing ghost levels to ensure correct plotting/analyses\n",
    "\n",
    "ggplot(data = rsub_t, aes(x = ind, y = r2)) + geom_boxplot()+theme_classic() #elev explains more variation in the transients\n",
    "\n",
    "####Variance Partitioning of Env Predictors####\n",
    "#would I be basing my total remaining unexplained variation off of the meanOcc~logA relationship? (OA.i?)\n",
    "#so the 12% remaining\n",
    "#focusing just on OA.i and main env vars\n",
    "#how do variance partitioning with more than 4 parts? \n",
    "\n",
    "globalmod<-lm(OA.i~elev+meanP+temp+ndvi, data=env_coefs)\n",
    "mod1<-lm(OA.i~elev, data=env_coefs)\n",
    "mod2<-lm(OA.i~meanP, data=env_coefs)\n",
    "mod3<-lm(OA.i~ndvi, data=env_coefs)\n",
    "mod4<-lm(OA.i~temp, data=env_coefs)\n",
    "#and then Euclid_mod2\n",
    "summary(globalmod)$r.squared\n",
    "summary(mod1)$r.squared\n",
    "summary(mod2)$r.squared\n",
    "summary(mod3)$r.squared\n",
    "summary(mod4)$r.squared \n",
    "\n",
    "\n",
    "#running with mods 2+3 bc best ranked and most interesting \n",
    "a= summary(globalmod)$r.squared - summary(mod2)$r.squared\n",
    "a\n",
    "c= summary(globalmod)$r.squared - summary(mod3)$r.squared\n",
    "c\n",
    "b= summary(mod2)$r.squared - c\n",
    "b\n",
    "d= 1- summary(globalmod)$r.squared\n",
    "d\n",
    "#isn't it ok that d = ~87.5% tho, given that the r^2 for occ~logA was 88%? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
